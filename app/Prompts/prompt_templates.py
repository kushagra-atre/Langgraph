from langchain_core.prompts import ChatPromptTemplate
from langchain import hub

class Prompts:
    RAG_PROMPT = hub.pull("rlm/rag-prompt")

    HALLUCINATION_GRADER_SYSTEM = (
    """
    You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.
    Give a binary score 'yes' or 'no', where 'yes' means that the answer is grounded in / supported by the set of facts.

    IF the generation includes code examples, make sure those examples are FULLY present in the set of facts, otherwise always return score 'no'.
    """
    )
    HALLUCINATION_GRADER_PROMPT = ChatPromptTemplate.from_messages(
        [
            ("system", HALLUCINATION_GRADER_SYSTEM),
            ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
        ]
    )

    ANSWER_GRADER_SYSTEM = (
    """
    You are a grader assessing whether an answer addresses / resolves a question.
    Give a binary score 'yes' or 'no', where 'yes' means that the answer resolves the question.
    """
    )
    ANSWER_GRADER_PROMPT = ChatPromptTemplate.from_messages(
        [
            ("system", ANSWER_GRADER_SYSTEM),
            ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
        ]
    )

    QUERY_REWRITER_SYSTEM = (
    """
    You a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval.
    Look at the input and try to reason about the underlying semantic intent / meaning.
    """
    )
    QUERY_REWRITER_PROMPT = ChatPromptTemplate.from_messages(
        [
            ("system", QUERY_REWRITER_SYSTEM),
            (
                "human",
                "Here is the initial question: \n\n {question} \n Formulate an improved question.",
            ),
        ]
    )
